{"cells":[{"cell_type":"markdown","source":["# Introduction to Apache Spark \n\n### Spark Tutorials - Part I\n  \n### Agenda\n* Loading data\n* Spark SQL & basic data transformations\n* Writing data\n* Caching data for performance"],"metadata":{}},{"cell_type":"markdown","source":["## 1.0 Loading data\n### 1.1 Initialising access to AWS S3"],"metadata":{}},{"cell_type":"code","source":["# All client data should be residing on Sagacity's AWS S3 storage\n# We need to setup access to the the location on S3 \n\n# Set the bucket name\nS3_BUCKET = \"sagacity-analyst-scratch\"\n\n# Pull your AWS access keys from the Databricks 'secrets' secure vault\n# You will be given AWS keys by security who will install these into the 'secrets' vault\nS3_PATH = \"s3a://%s:%s@%s/\" % (dbutils.secrets.get(\"cstephenson\", \"access_key\"), dbutils.secrets.get(\"cstephenson\", \"secret_key\").replace(\"/\", \"%2F\"), S3_BUCKET)\n\n# NOTE: You should NEVER store your AWS Access Keys in Notebooks or anywhere else that is not secure"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# What has this done?\n# For accessing files on S3 you must specify the path as:\n# s3a://[AWS_ACCESS_KEY]:[AWS_SECRET_KEY]@[S3_BUCKET_NAME]/[FOLDER_NAME]/[FILE_NAME]\n\nprint(S3_PATH)\n# Note your AWS keys are have been \"REDACTED\" but behind the scenes Spark is passing your keys through to AWS to enable your \n# request for the data to be authenticated  "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### 1.2 Loading data from S3"],"metadata":{}},{"cell_type":"code","source":["# Define the filename and its location within the S3 bucket\nfile_path = S3_PATH + 'zeppelin_demo/appl_stock.csv'\nprint(file_path)\n\n# Load TT inputs file from S3\n# Format 'CSV' - The file is a field delimited text file\n# Option 'delimiter' - The file is delimited by a comma ',' note tab delimited files are indicated by '\\t'\n# Option 'header' - If 'True' the first row of the file contains header information (column names)\n# Option 'inferSchema' - If 'True' Spark will attempt to 'guess' the data types for each column \n# Load - Provide the file path/location of the file\ndf_apple_stock = spark.read.format('csv'\n                                  ).option('delimiter', ','\n                                  ).option('header', 'true'\n                                  ).option('inferSchema', 'true'\n                                  ).load(file_path)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#todo\n[TAB] for code-completion\ndf_apple_stock."],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["More on the various formats and options for loading files can be found here\n[Databricks Documentation - Read CSV](https://docs.databricks.com/spark/latest/data-sources/read-csv.html)"],"metadata":{}},{"cell_type":"markdown","source":["### 1.3 Displaying data"],"metadata":{}},{"cell_type":"code","source":["# Let's take a look at the dataframe contents ...\n# Notice the default views - Grid, Charts, or Export to CSV\ndisplay(df_apple_stock)\n\n# NOTE: The grid will only display the first 1000 rows in preview and the first 50 columns!\n#       You can also download the results for analysis locally - Be careful when downloading customer sensitive data!"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Some other methods of viewing the data - less pretty\ndf_apple_stock.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# View columns and datatypes - you can also click the icon for the dataframe at the footer of the cell\ndf_apple_stock.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Display summary statistics on the table columns (stats for numeric values)\ndisplay(df_apple_stock.summary())"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Loading data quickly from your client (with health warning)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### 1.4 Loading other file formats"],"metadata":{}},{"cell_type":"code","source":["# GZ files\n# --------\nfile_path = S3_PATH + '/zeppelin_demo/titanic.gz'\n\n# Spark can read files compressed in the GZIP format directly into a dataframe\n# It's good practice, especially if the original text file is large, to ask the provider to compress it first before uploading on to S3\ndf_titanic = spark.read.format('csv'\n                                  ).option('delimiter', ','\n                                  ).option('header', 'true'\n                                  ).option('inferSchema', 'true'\n                                  ).load(file_path)\n\n# Parameter 'n' to show the first 'n' rows of the dataframe, 'truncate'=False will show entire column width \ndf_titanic.show(n=5, truncate=True)\n\n# Note: the file suffix needs to be '.GZ' for this to work in spark, if the file is suffixed '.GZIP' you will need to rename it"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Parquet files\n# -------------\n# Parquet files are a schema aware, compressed file format based on columnar storage \n# It is highly recommended to store all your data in Parquet format as it takes less space, and is MUCH faster to load\n# If you recieve a file from a client, one of the first things you should do is conert it to Parquet  \nfile_path = S3_PATH + '/zeppelin_demo/appl_stock_open.parquet'\n\ndf_apple_open = spark.read.format('parquet').load(file_path)\ndf_apple_open.show(n=5)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["More on the the Parquet file format with examples of working with them on Spark here\n[Parquet Example](https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/)"],"metadata":{}},{"cell_type":"code","source":["# JSON files\n# Spark can support nested data structures and read data directly on these - JSON is a commonly used format here\n# See  also YAML, XML\nfile_path = S3_PATH + '/zeppelin_demo/socrata_metadata_311.json'\n\ndf_socrata = spark.read.format('json').load(file_path)\n\ndisplay(df_socrata)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df_socrata.printSchema()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["###1.5 Using Schemas"],"metadata":{}},{"cell_type":"code","source":["  from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n\n  titanic_schema = StructType([\n     StructField('passenger_id', IntegerType())\n  ,  StructField('survived', IntegerType())\n  ,  StructField('p_class', IntegerType())\n  ,  StructField('name', StringType())\n  ,  StructField('sex', StringType())\n  ,  StructField('age', DoubleType())\n  ,  StructField('sib_sp', IntegerType())\n  ,  StructField('p_arch', IntegerType())\n  ,  StructField('ticket', StringType())\n  ,  StructField('fare', DoubleType())\n  ,  StructField('cabin', StringType())\n  ,  StructField('embarked', StringType())\n  ])\n\n  file_path = S3_PATH + '/zeppelin_demo/titanic.gz'\n\n  df_titanic = spark.read.format('csv').option('delimiter', ','\n                                    ).option('header', 'true'\n                                    ).schema(titanic_schema\n                                    ).load(file_path)\n\n  display(df_titanic)    "],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Loading non-standard file formats - load the data into a single column\n# use the format 'text'\nfile_path = S3_PATH + '/zeppelin_demo/apple_stock.rpt'\n\n# will load the full file splitting the rows on the new line character '\\n' all row data in a single column\ndf_apple_stock_rpt = spark.read.format('text').load(file_path)\n\ndisplay(df_apple_stock_rpt)\n# Note. for a solution to load this data properly into a dataframe with the correct column data, see Appendix below "],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Note\nSpark can also load data directly from relational and NoSQL databases (using JDBC) e.g. Oracle, MySQL, Cassandra etc..."],"metadata":{}},{"cell_type":"markdown","source":["## 2.0 Spark SQL & basic data transformations\n\nOnce you have data loaded into your dataframe you can perform your data transformations:\n* The DataFrame API with Python, Scala or R\n* Using SQL with the Spark SQL API"],"metadata":{}},{"cell_type":"markdown","source":["### 2.1 Spark SQL API"],"metadata":{}},{"cell_type":"code","source":["# Register as a Temporary View - As it is temporary, this is ephemeral and the data in the table will be lost once your Spark cluster is terminated \ndf_titanic.createOrReplaceTempView('titanic')"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%sql\n-- Now you can use SQL interactively using the %sql hint (magic) at the top of the cell\n-- You can now query the table you registered earlier\n\n-- Note: in-line comments in SQL are '--'\n/* ...to start\nmult-line comments,\nand to finish... */\nSELECT\n  Sex\n, COUNT(*) AS vol_total\n, SUM(Survived) AS vol_survived\n, ROUND(SUM(Survived) / COUNT(*), 2)  AS pct_survived\nFROM\n  titanic\nGROUP BY\n  Sex"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#### Note. Local vs. Global tables   \n**Local**   \n> Only accessible by your user only and only on the current Spark cluster. It will also be automatically **deleted** when the Spark cluster is terminated  \n> ```df.createOrReplaceTempView('my_local_table') ```  \n  \n**Global**  \n> Accessible to all users across all Spark clusters (with the correct permissions assigned), and will be **saved** after the Spark cluster is terminated  \n> ```df.createOrReplaceTable('my_global_table') ```  \n**Generally the abilibilty to create Global tables will be denied to most users!**\n\nIf you are more familiar with SQL DDL syntax these can also be used  \n[Spark SQL DDL Documentation here](https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-table.html#)"],"metadata":{}},{"cell_type":"code","source":["# You can also execute SQL pulling the results back directly into a new Dataframe\n\n# Note Python multi-line string with three double quotes \"\"\" \"\"\" or single quotes ''' '''\nsql_text = \"\"\"\nSELECT\n  Sex\n, COUNT(*) AS vol_total\n, SUM(Survived) AS vol_survived\n, ROUND(SUM(Survived) / COUNT(*), 2)  AS pct_survived\nFROM\n  titanic\nGROUP BY\n  Sex\n\"\"\"\n\n# SparkSession.sql(sqlQuery)\n#  Returns a DataFrame representing the result of the given query.\n# Note: Databricks makes your SparkSession automattically availble via the variable 'spark'\ndf_srv_sex = spark.sql(sql_text)\n\ndf_srv_sex.show()\n\n\n# Or you can use the Python line continuation character '\\'\nsql_text = 'SELECT' \\\n' Sex' \\\n' , COUNT(*) AS vol_total' \\\n' , SUM(Survived) AS vol_survived' \\\n' , ROUND(SUM(Survived) / COUNT(*), 2) AS pct_survived' \\\n' FROM ' \\\n'  titanic ' \\\n' GROUP BY ' \\\n'  Sex'\n\ndf_srv_sex = spark.sql(sql_text)\ndf_srv_sex.show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Spark SQL reference here [Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)"],"metadata":{}},{"cell_type":"markdown","source":["### 2.2 Pyspark API (Python)"],"metadata":{}},{"cell_type":"code","source":["# Equivalent query using the DataFrame API and Python ...\n\n# We first need to import the functions we want to use ...\nfrom pyspark.sql.functions import count, sum, round\n\ndf_titanic.select(\n  # Need to select all the columns we will be displaying or using\n  'Sex'\n, 'Survived'\n).groupBy(\n  # The column(s) we are grouping over here\n  'Sex'\n).agg(\n  # .agg() here are our aggrgation functions & calculations, note the use of .alias() to set the column name, equivalent to \"AS\" in SQL\n  count('*').alias('vol_total') \n, sum('Survived').alias('vol_survived') \n, round(sum('Survived') / count('*'), 2).alias('vol_survived') \n).show() # finally show() the results\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["###2.3 Revisiting file load - Loading difficult files"],"metadata":{}},{"cell_type":"code","source":["# A common problem is initial clensing aof data and ensuring datatypes are correct\n# A common solution is to intially the load data into String/Character columns and clean/cast the data directly in the DataFrame\n# Lets look at solving this using the S\n\n# Here we are loading without the 'inferSchema' option which will interpret every column as a StringType\ndf_citations = spark.read.format('csv'\n                                ).option('delimiter', ','\n                                ).option('header', 'true'\n                                ).load(S3_PATH + '/zeppelin_demo/parking-citations.gz')"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Transformation and datatype conversion in dataframe columns\nfrom pyspark.sql.functions import col \nfrom pyspark.sql.types import DateType, DoubleType\n\n# withColumn method is the pyspark way of altering the data in a dataframe column\n# the 'col' function returns the column object \n# the 'cast' method allow conversion from one data type to another\n\n# Cast Latitude to DoubleType\ndf_citations = df_citations.withColumn('Latitude', col('Latitude').cast(DoubleType()))\n\n# Cast Longitude to DoubleType\ndf_citations = df_citations.withColumn('Longitude', col('Longitude').cast(DoubleType()))\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Notice the presence of nulls in 'Fine Amount'\ndisplay(\n  df_citations.select('*').filter(col('Fine Amount').isNull())\n)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# More complex transformations ...\nfrom pyspark.sql.functions import substring, when\n\n# using substring function\n# Transform \"Issue Date\" removing the time portion to obtain a valid date format and cast to DateType\ndf_citations = df_citations.withColumn('Issue Date', substring('Issue Date', 1, 10).cast(DateType()))\n\n# using na.fill method \n# Transform \"Fine amount\" to change NULL values to 0.0 \ndf_citations = df_citations.na.fill('0.0', 'Fine Amount')\n\n# ... and cast column to a DoubleType\ndf_citations = df_citations.withColumn('Fine Amount', col('Fine Amount').cast(DoubleType()))\n"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["display(df_citations)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# The expr() function\nfrom pyspark.sql.functions import expr\n# If you know the SQL function equivalent you can use the expr() function to build SQL functions these in you Python code\n\n# eqivalent of ```df_citations = df_citations.withColumn('Issue Date', substring('Issue Date', 1, 10).cast(DateType()))```\ndf_citations = df_citations.withColumn('Issue Date', expr(\"CAST(SUBSTRING('Issue Date', 1, 10) AS DATE)\"))\n"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["Python strings - if you need to use quotes in a quoted string - you can enclose single '' in \"\" or vice-versa, or use the blackslash \\ escape character ...\n```df_citations = df_citations.withColumn('Issue Date', expr('CAST(SUBSTRING(\\'Issue Date\\', 1, 10) AS DATE)'))```"],"metadata":{}},{"cell_type":"markdown","source":["#### A note on Methods and Functions\n\n##### Functions\nFunctions are directly callable, take arguments/inputs and return an output<br/>\n[Pyspark.sql.functions documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)<br/>\n##### Methods\nMethods are like functions but operate on an object<br/>\ne.g. all the methods that can operate on the the Column object<br/>\n[Pyspark.sql.column documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n\n\nSometimes we can choose method or function to achieve the same result\nFunction version - Pyspark.sql.functions.substring()<br/>\n```df_citations = df_citations.withColumn('Issue Date', substring('Issue Date', 1, 10).cast(DateType()))```\n\nMethod version - Pyspark.sql.column.substr()<br/>\n```df_citations = df_citations.withColumn('Issue Date', col('Issue Date').substr(1, 10).cast(DateType()))```"],"metadata":{}},{"cell_type":"markdown","source":["What is Markdown?\n[Markdown Reference](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)"],"metadata":{}},{"cell_type":"markdown","source":["## 3.0 Writing data"],"metadata":{}},{"cell_type":"markdown","source":["### 3.1 File formats"],"metadata":{}},{"cell_type":"code","source":["\n# Write back as PARQUET (RECOMMENDED)\ndf_citations.write.mode('overwrite').format('parquet').save(S3_PATH + '/zeppelin_demo/test_out.parquet') \n\n# Write back as CSV with a header and pipe delimited fields\ndf_citations.write.mode('overwrite'       # option to 'append' data as well\n             ).format('csv'\n             ).option('delimiter', '|'\n             ).option('header', 'true'\n             ).save(S3_PATH + '/zeppelin_demo/test_out.csv') \n\n# Note: for larger files, Spark 'partitions' the data. This is how spark 'scales-out' processing  by work on parts of the dataset in parallel \n# Note also the difference in size between compressed PARQUET format and regular CSV, 665 KB vs 4.8 MB"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### 3.2 Partitioning  \nSpark 'partitions' it's dataframes in order to gain the speed benifits of processing the dataset in parallel. i.e. each thread/process on the cluster will process a single partition in parallel to all the other available processes, each working on their own partition file.\n\nHowever, what if I just want to view the file in Excel or other tool that doesn't recognise partitions?"],"metadata":{}},{"cell_type":"code","source":["# Write back to a single CSV file \n\n# Option 1 - Collesce (write to a single partition)\ndf_titanic.coalesce(1).write.mode('overwrite'       # option to 'append' data as well\n             ).format('csv'\n             ).option('delimiter', ','\n             ).option('header', 'true'\n             ).save(S3_PATH + '/zeppelin_demo/titanic_spark.csv') \n\n\n# Option 2 - Use Pandas and S3FS\nimport pandas\nimport s3fs\n\n# Convert Spark dataframe to Pandas dataframe\npdf_titanic = df_titanic.toPandas()\nbytes_to_write = pdf_titanic.to_csv(None).encode()\n\n\n# Mount S3 as a filesystem (need to provide your access keys)\nfs = s3fs.S3FileSystem(key=dbutils.secrets.get(\"cstephenson\", \"access_key\"), secret=dbutils.secrets.get(\"cstephenson\", \"secret_key\").replace(\"/\", \"%2F\"))\n\n# Open a CSV file for writing ('wb') on S3\n\nwith fs.open('sagacity-analyst-scratch/zeppelin_demo/titanic_pandas.csv', 'wb') as titanic_file:\n    # write the file bytes as a CSV\n    titanic_file.write(bytes_to_write)\n"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### More documentation ...  \n[Pandas Documentation here](https://pandas.pydata.org/pandas-docs/stable/index.html)  \n[S3FS Documentation here](https://s3fs.readthedocs.io/en/latest/)"],"metadata":{}},{"cell_type":"code","source":["# partitionBy\n#\n# splits the files into sub-folders and therefore potentially reducing the number of files\n# that would need to be accessed for a query - known as \"partition pruning\"\n\n# add new column with the year and month of the ticket issue\ndf_citations = df_citations.withColumn(\"issue_year\", expr(\"YEAR(`Issue Date`)\"))\n\n# partition by the new issue reporting period\ndf_citations.write.partitionBy('issue_year'\n             ).mode('overwrite'\n             ).format('csv'\n             ).save(S3_PATH + '/zeppelin_demo/citations_by_year.csv') \n"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["#### Note: \nYou should be thinking up-front about how your data are likely to be accessed before choosing what column to partition by. \n\nAn example of a good candidate for this would be a column that contains a reporting period, assuming one typically queries one reporting period at a time - this could make a big difference to how long your query takes to execute as your data grows in size.  \n\nOther considerations ...  \n\n__Partition Skew__\nIdeally, all your partitions should be of roughly equal size otherwise some processes will do an unfair amount of processing (on the largest partitions) while others will complete quickly resulting in an overall longer execution time. If you partition your dataset on an un-evenly distributed column, you might get partition skew when your query needs to run across those partitions.   \nBy default, if no partitioning column is selected, Spark will partition randomly ensuring an even distribution\n\n__Number of Partitions__\nAs Spark gains it's performance benfits by running its queries in parallel (one process running over a single partition) you need to make sure that there are enough partitions in your data to benifit from the parallelism provided by the cluster balanced against the partitioning scheme to only access the partitions that are needed!  \n\nThere are a lot of other 'tweaks' to ensure your data are processed efficeintly, if you need help tuning queries please ask :)"],"metadata":{}},{"cell_type":"code","source":["# check your skew\ndisplay(df_citations.select('issue_year').groupBy('issue_year').count())"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["## 4.0 Caching data for performance\n\nReading and writing data from/to S3 can be a bit slow and inefficent. We will also incur a small amount of AWS data transfer costs for every read/write from S3.\n\nIt is therefore good practice to 'Cache' your data sets locally (on the Spark cluster) if you are going to be running more than a few queries"],"metadata":{}},{"cell_type":"markdown","source":["### 4.1 Persist and Unpersist"],"metadata":{}},{"cell_type":"code","source":["# 4.1 Demonstrate query performance on caches and uncached data\nfrom pyspark import StorageLevel\n\ndf_citations.persist(StorageLevel.MEMORY_AND_DISK)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["### 4.2 Lazy Evaluation"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["## Appendix"],"metadata":{}},{"cell_type":"code","source":["#\n# Example - Reading in 'fixed-width' files\n#\nfrom pyspark.sql.functions import lit, col, trim\nfrom pyspark.sql.types import DateType, DoubleType, IntegerType\n\n\n# Load the file as text, i.e. the full row into one column \ndf_txt_rpt = spark.read.format('text').load(S3_PATH + '/zeppelin_demo/apple_stock.rpt')\ndf_txt_rpt.show(n=5, truncate=False)\n\ndf_apple_stock = df_txt_rpt.select( \n  # use substrings to pull out the elements we need ....\n  trim(col('value').substr(2,19)).cast(DateType()).alias('date')\n, trim(col('value').substr(22,18)).cast(DoubleType()).alias('open')\n, trim(col('value').substr(41,18)).cast(DoubleType()).alias('high')\n, trim(col('value').substr(60,18)).cast(DoubleType()).alias('low')\n, trim(col('value').substr(79,18)).cast(DoubleType()).alias('close')\n, trim(col('value').substr(98,9)).cast(IntegerType()).alias('vol')\n, trim(col('value').substr(108,18)).cast(DoubleType()).alias('adj_close')\n).filter(\n  # Filter out unwanted rows - separators and header row\n  (col('value').substr(0,1)==lit('|')) \n  & (trim(col('value').substr(2,19)).alias('date')!=lit('Date'))\n)\n\ndf_apple_stock.show(n=5, truncate=False)"],"metadata":{},"outputs":[],"execution_count":59}],"metadata":{"name":"Spark Intro - I","notebookId":95759},"nbformat":4,"nbformat_minor":0}
